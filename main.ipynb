{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## University of Bari Aldo Moro\n",
    "Master Degree in <b>Computer Science</b> - <b>Computer Vision Course</b><br>\n",
    "Francesco Peragine - f.peragine@studenti.uniba.it"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vision Transformer for Unmanned Aerial Vehicles Agronomic Research\n",
    "- [Dependencies](#dependencies)\n",
    "- [Libraries](#libraries)\n",
    "- [Dataset](#dataset)\n",
    "    - [Parameters](#parameters)\n",
    "    - [GCP Finder](#gcp_finder)\n",
    "    - [Plots Clipper](#plots_clipper)\n",
    "    - [Ground Truth](#ground-truth)\n",
    "    - [Creation](#creation)\n",
    "    - [Loading](#loading)\n",
    "    - [Splitting](#splitting)\n",
    "- [Dataloader](#dataloader)\n",
    "- [Model](#model)\n",
    "    - [Training](#training)\n",
    "    - [Evaluation](#evaluation)\n",
    "    - [Save](#save)\n",
    "    - [Prediction](#prediction)\n",
    "- [Visualization](#visualization)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "<a name='dependencies'></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "<a name='libraries'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install git+https://github.com/PyTorchLightning/pytorch-lightning\n",
    "import pytorch_lightning as pl\n",
    "print(pl.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as TF\n",
    "import torch\n",
    "import pandas as pd\n",
    "import mlflow.pytorch\n",
    "import mlflow\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.models import get_model, ViT_L_32_Weights\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paths\n",
    "<a name=\"paths\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available and being used\", device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available, using CPU instead\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"./data/raw/Case_Study_1/Raw_Images\"\n",
    "GCP_PATH = \"./data/GCP_Images\"\n",
    "ORTHOMOSAIC_PATH = \"./data/orthophoto/raster.tif\"\n",
    "SHAPEFILE_PATH = \"./data/raw/Case_Study_1/Shapefile/Plots_Shapefile/all_plots.shp\"\n",
    "PLOT_PATH = \"./data/plots\"\n",
    "GROUND_TRUTH_PATH = './data/ground_truth/ground_truth.csv'\n",
    "CHECKPOINT_PATH = \"./data/checkpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = pd.read_csv(GROUND_TRUTH_PATH)\n",
    "print(ground_truth)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "<a name='preprocessing'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCP Finder\n",
    "<a name='gcp_finder'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.gcp_finder import GCPFinder\n",
    "\n",
    "# gcp_finder = GCPFinder(DATASET_PATH, GCP_PATH)\n",
    "# gcp_finder.gcp_mover()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots Clipper\n",
    "<a name='plots_clipper'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.clipper import Clipper\n",
    "   \n",
    "# clipper = Clipper(ORTHOMOSAIC_PATH, SHAPEFILE_PATH, PLOT_PATH)\n",
    "# clipper.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.start_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "<a name=\"dataset\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hue = 0.1\n",
    "saturation = 0.1\n",
    "brightness = 0.1\n",
    "contrast = 0.2\n",
    "\n",
    "transforms = v2.Compose([\n",
    "    v2.RandomHorizontalFlip(),\n",
    "    v2.RandomVerticalFlip(),\n",
    "    \n",
    "    v2.ColorJitter(hue=hue, saturation=saturation, brightness=brightness, contrast=contrast), # Random color jittering for slight changes in hue, saturation, brightness, contrast\n",
    "\n",
    "    v2.RandomApply([v2.RandomErasing(p=0.2, scale=(0.01, 0.1), ratio=(0.3, 3.3))], p=0.3), # Randomly add shadow patterns\n",
    "    # v2.RandomApply([v2.ColorJitter(brightness=0.3, contrast=0.3, saturation=0, hue=0)], p=0.2), # Randomly darken image corners to simulate vignetting\n",
    "    # v2.RandomApply([v2.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.5, hue=0.3)], p=0.1), # Randomly add lens flare effects\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.dataset import PlotsDataset\n",
    "from lib.utils import calculate_mean_std\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "IMG_SIZE = (224,224)\n",
    "\n",
    "base_dataset = {\n",
    "    \"name\": \"base\",\n",
    "    \"dataset\": PlotsDataset(ground_truth, PLOT_PATH, IMG_SIZE),\n",
    "}\n",
    "augmented_dataset = {\n",
    "    \"name\": \"augmented\",\n",
    "    \"dataset\": PlotsDataset(ground_truth, PLOT_PATH, IMG_SIZE, transforms),\n",
    "}\n",
    "\n",
    "curr_dataset = augmented_dataset\n",
    "\n",
    "# dataset = ConcatDataset([base_dataset, augmented_dataset])\n",
    "dataset = curr_dataset[\"dataset\"]\n",
    "\n",
    "# means, stds = calculate_mean_std(combined_dataset)\n",
    "# print(f\"Means: {means}, Stds: {stds}\")\n",
    "\n",
    "# normalize_transforms = v2.Compose([v2.Normalize(mean=means, std=stds)])\n",
    "\n",
    "# normalized_images = [torch.clamp(normalize_transforms(sample[0]), 0, 1) for sample in combined_dataset]\n",
    "# labels = [sample[1] for sample in combined_dataset]\n",
    "\n",
    "# dataset = TensorDataset(torch.stack(normalized_images), torch.Tensor(labels))\n",
    "\n",
    "\n",
    "print(f\"Dataset length: {len(dataset)}\")\n",
    "print(f\"Image type: {type(dataset[0][0])}\")\n",
    "print(f\"Image shape: {dataset[0][0].shape})\")\n",
    "print(f\"Label type: {type(dataset[0][1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils import calculate_mean_std\n",
    "\n",
    "means, stds = calculate_mean_std(dataset)\n",
    "print(f\"Means: {means}, Stds: {stds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(dataset[0][0].permute(1,2,0))\n",
    "plt.title(\"Normalized image - crop height: \" + str(dataset[0][1])+ \"m\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pixel distribution\n",
    "plt.hist(dataset[0][0].permute(1,2,0).ravel(), bins=50, density=True)\n",
    "plt.xlabel(\"Pixel values\")\n",
    "plt.ylabel(\"Relative frequency\")\n",
    "plt.title(\"Distribution of pixels\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split\n",
    "<a name='split'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_SIZE = 0.2\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "temp_dataset, test_dataset = train_test_split(dataset, test_size=TEST_SIZE)\n",
    "train_dataset, val_dataset = train_test_split(temp_dataset, test_size=VAL_SIZE)\n",
    "\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")\n",
    "print(f\"Test set size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "<a name='hyperparameters'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_EPOCS = 30\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "WORKERS = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaders\n",
    "<a name=\"dataloader\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=WORKERS)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=WORKERS)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=WORKERS)\n",
    "\n",
    "print(f\"Train Dataloader size: {len(train_loader)}\")\n",
    "print(f\"Validation Dataloader size: {len(val_loader)}\")\n",
    "print(f\"Test Dataloader size: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils import get_dataset_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dataset_samples(base_dataset[\"dataset\"], \"Base dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dataset_samples(augmented_dataset[\"dataset\"], \"Augmented dataset\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model\n",
    "<a name='model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.uav_vit import UAV_vit\n",
    "\n",
    "# Load the ViT model\n",
    "model_name = \"vit_l_32\"\n",
    "model_weights = ViT_L_32_Weights.IMAGENET1K_V1\n",
    "\n",
    "loss_fn = TF.mse_loss\n",
    "vit_l_32 = get_model(model_name, weights=model_weights)\n",
    "\n",
    "model = UAV_vit(vit_l_32, loss_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "<a name='training'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a trainer\n",
    "trainer = pl.Trainer(max_epochs=TRAIN_EPOCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "<a name='prediction'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model, test_loader, verbose=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.log_param(\"model\", f\"{model_name} - {model_weights}\")\n",
    "mlflow.log_param(\"dataset\", curr_dataset[\"name\"])\n",
    "mlflow.log_param(\"dataset_length\", len(dataset))\n",
    "mlflow.log_param(\"batch_size\", BATCH_SIZE)\n",
    "mlflow.log_param(\"train_epochs\", TRAIN_EPOCS)\n",
    "mlflow.log_param(\"hue\", hue)\n",
    "mlflow.log_param(\"saturation\", saturation)\n",
    "mlflow.log_param(\"brightness\", brightness)\n",
    "mlflow.log_param(\"contrast\", contrast)\n",
    "mlflow.end_run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization\n",
    "<a name='visualization'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.visualizer import Visualizer\n",
    "\n",
    "# Instantiate the visualizer\n",
    "visualizer = Visualizer()\n",
    "\n",
    "# Use the visualizer to plot the desired graphs\n",
    "# visualizer.plot_loss(train_losses, val_losses)\n",
    "# visualizer.plot_predictions(true_values, predicted_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
