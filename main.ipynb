{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## University of Bari Aldo Moro\n",
    "Master Degree in <b>Computer Science</b> - <b>Computer Vision Course</b><br>\n",
    "Francesco Peragine - f.peragine@studenti.uniba.it"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vision Transformer for Unmanned Aerial Vehicles Agronomic Research\n",
    "- [Dependencies](#dependencies)\n",
    "- [Libraries](#libraries)\n",
    "- [Dataset](#dataset)\n",
    "    - [Parameters](#parameters)\n",
    "    - [GCP Finder](#gcp_finder)\n",
    "    - [Plots Clipper](#plots_clipper)\n",
    "    - [Ground Truth](#ground-truth)\n",
    "    - [Creation](#creation)\n",
    "    - [Loading](#loading)\n",
    "    - [Splitting](#splitting)\n",
    "- [Dataloader](#dataloader)\n",
    "- [Model](#model)\n",
    "    - [Training](#training)\n",
    "    - [Evaluation](#evaluation)\n",
    "    - [Save](#save)\n",
    "    - [Prediction](#prediction)\n",
    "- [Visualization](#visualization)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "<a name='dependencies'></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "<a name='libraries'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install git+https://github.com/PyTorchLightning/pytorch-lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries\n",
    "<a name=\"libraries\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as TF\n",
    "import torch\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision.models import get_model, ViT_L_32_Weights\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "from notebooks.utils import calculate_mean_std, get_dataset_samples\n",
    "from notebooks.dataset import PlotsDataset\n",
    "from notebooks.gcp_finder import GCPFinder\n",
    "from notebooks.clipper import Clipper\n",
    "from notebooks.uav_vit import UAV_vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pl.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paths\n",
    "<a name=\"paths\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available and being used\", device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available, using CPU instead\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"./data/raw/Case_Study_1/Raw_Images\"\n",
    "GCP_PATH = \"./data/GCP_Images\"\n",
    "GROUND_TRUTH_PATH = './data/ground_truth/ground_truth.csv'\n",
    "MODEL_PATH = \"./data/models/\"\n",
    "ORTHOMOSAIC_PATH = \"./data/orthophoto/raster.tif\"\n",
    "PLOT_PATH = \"./data/plots\"\n",
    "SHAPEFILE_PATH = \"./data/raw/Case_Study_1/Shapefile/Plots_Shapefile/all_plots.shp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ground truth\n",
    "<a name=\"ground_truth\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading\n",
    "ground_truth = pd.read_csv(GROUND_TRUTH_PATH)\n",
    "\n",
    "# Elevation format conversion to float32\n",
    "num_format = \"float32\"\n",
    "ground_truth[\"elev\"] = ground_truth[\"elev\"].astype(num_format).values\n",
    "print(type(ground_truth[\"elev\"].values[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "<a name='preprocessing'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCP Finder\n",
    "<a name='gcp_finder'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gcp_finder = GCPFinder(DATASET_PATH, GCP_PATH)\n",
    "# gcp_finder.gcp_mover()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots Clipper\n",
    "<a name='plots_clipper'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clipper = Clipper(ORTHOMOSAIC_PATH, SHAPEFILE_PATH, PLOT_PATH)\n",
    "# clipper.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.start_run()\n",
    "mlflow.pytorch.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "<a name=\"dataset\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hue = 0.1\n",
    "saturation = 0.1\n",
    "brightness = 0.1\n",
    "contrast = 0.2\n",
    "\n",
    "transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    \n",
    "    transforms.ColorJitter(hue=hue, saturation=saturation, brightness=brightness, contrast=contrast), # Random color jittering for slight changes in hue, saturation, brightness, contrast\n",
    "\n",
    "    transforms.RandomApply([transforms.RandomErasing(p=0.2, scale=(0.01, 0.1), ratio=(0.3, 3.3))], p=0.3), # Randomly add shadow patterns\n",
    "    # transforms.RandomApply([transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0, hue=0)], p=0.2), # Randomly darken image corners to simulate vignetting\n",
    "    # transforms.RandomApply([transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.5, hue=0.3)], p=0.1), # Randomly add lens flare effects\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (224,224)\n",
    "\n",
    "base_dataset = {\n",
    "    \"name\": \"base\",\n",
    "    \"dataset\": PlotsDataset(ground_truth, PLOT_PATH, IMG_SIZE),\n",
    "}\n",
    "augmented_dataset = {\n",
    "    \"name\": \"augmented\",\n",
    "    \"dataset\": PlotsDataset(ground_truth, PLOT_PATH, IMG_SIZE, transforms),\n",
    "}\n",
    "\n",
    "concat_dataset = {\n",
    "    \"name\": \"concat\",\n",
    "    \"dataset\": ConcatDataset([base_dataset[\"dataset\"], augmented_dataset[\"dataset\"]])\n",
    "}\n",
    "\n",
    "curr_dataset = concat_dataset\n",
    "\n",
    "# normalize_transforms = transforms.Compose([transforms.Normalize(mean=means, std=stds)])\n",
    "\n",
    "# normalized_images = [torch.clamp(normalize_transforms(sample[0]), 0, 1) for sample in combined_dataset]\n",
    "# labels = [sample[1] for sample in combined_dataset]\n",
    "\n",
    "# dataset = TensorDataset(torch.stack(normalized_images), torch.Tensor(labels))\n",
    "\n",
    "print(f\"Dataset length: {len(curr_dataset['dataset'])}\")\n",
    "print(f\"Image type: {type(curr_dataset['dataset'][0][0])}\")\n",
    "print(f\"Image shape: {curr_dataset['dataset'][0][0].shape})\")\n",
    "print(f\"Label type: {type(curr_dataset['dataset'][0][1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means, stds = calculate_mean_std(curr_dataset['dataset'])\n",
    "print(f\"Means: {means}, Stds: {stds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(curr_dataset['dataset'][0][0].permute(1,2,0))\n",
    "plt.title(\"Normalized image - crop height: \" + str(curr_dataset['dataset'][0][1])+ \"m\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pixel distribution\n",
    "plt.hist(curr_dataset['dataset'][0][0].permute(1,2,0).ravel(), bins=50, density=True)\n",
    "plt.xlabel(\"Pixel values\")\n",
    "plt.ylabel(\"Relative frequency\")\n",
    "plt.title(\"Distribution of pixels\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split\n",
    "<a name='split'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_SIZE = 0.2\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "dataset = {}\n",
    "dataset[\"tmp\"], dataset[\"test\"] = train_test_split(curr_dataset[\"dataset\"], test_size=TEST_SIZE)\n",
    "dataset[\"train\"], dataset[\"val\"] = train_test_split(dataset[\"tmp\"], test_size=VAL_SIZE)\n",
    "\n",
    "print(f\"Training set size: {len(dataset['train'])}\")\n",
    "print(f\"Validation set size: {len(dataset['val'])}\")\n",
    "print(f\"Test set size: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(dataset, phase):\n",
    "    # dataset is a dictionary with keys \"train\", \"val\", \"test\"\n",
    "    # each record in the dataset is a tuple of (image, label)\n",
    "    labels = [x[1] for x in dataset[phase]]\n",
    "    return labels\n",
    "\n",
    "phases = [\"train\", \"val\", \"test\"]\n",
    "labels = {}\n",
    "labels = {phase: get_labels(dataset, phase) for phase in phases}\n",
    "\n",
    "print(f\"Training set labels size: {len(labels['train'])}\")\n",
    "print(f\"Validation set labels size: {len(labels['val'])}\")\n",
    "print(f\"Test set labels size: {len(labels['test'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "<a name='hyperparameters'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_EPOCS = 50\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "WORKERS = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaders\n",
    "<a name=\"dataloader\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = {}\n",
    "dataloader[\"train\"] = DataLoader(dataset[\"train\"], batch_size=BATCH_SIZE, shuffle=True, num_workers=WORKERS)\n",
    "dataloader[\"val\"] = DataLoader(dataset[\"val\"], batch_size=BATCH_SIZE, shuffle=False, num_workers=WORKERS)\n",
    "dataloader[\"test\"] = DataLoader(dataset[\"test\"], batch_size=BATCH_SIZE, shuffle=False, num_workers=WORKERS)\n",
    "\n",
    "print(f\"Train Dataloader size: {len(dataloader['train'])}\")\n",
    "print(f\"Validation Dataloader size: {len(dataloader['val'])}\")\n",
    "print(f\"Test Dataloader size: {len(dataloader['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dataset_samples(base_dataset[\"dataset\"], \"Base dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dataset_samples(augmented_dataset[\"dataset\"], \"Augmented dataset\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model\n",
    "<a name='model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ViT model\n",
    "vit_l_32 = get_model(\"vit_l_32\", weights=ViT_L_32_Weights.IMAGENET1K_V1)\n",
    "loss_fn = TF.mse_loss\n",
    "\n",
    "model = UAV_vit(vit_l_32, loss_fn, labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "<a name='training'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor=\"val_loss\", patience=3, mode=\"min\")\n",
    "\n",
    "# Initialize a trainer\n",
    "trainer = pl.Trainer(\n",
    "    # default_root_dir=MODEL_PATH,\n",
    "    max_epochs=TRAIN_EPOCS,\n",
    "    # callbacks=[earlyStopping],\n",
    "    logger=True,\n",
    "    accelerator=\"auto\",\n",
    "    num_sanity_val_steps=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "trainer.fit(model, dataloader[\"train\"], dataloader[\"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.create_scatterplots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), MODEL_PATH + \"uav_vit.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "<a name='prediction'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "trainer.test(model, dataloader[\"test\"], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log the parameters\n",
    "mlflow.log_params({\"dataset\": curr_dataset[\"name\"], \"dataset_length\": len(curr_dataset[\"dataset\"])})\n",
    "mlflow.log_params({\"batch_size\": BATCH_SIZE, \"train_epochs\": TRAIN_EPOCS})\n",
    "mlflow.log_params({\"hue\": hue, \"saturation\": saturation, \"brightness\": brightness, \"contrast\": contrast})\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = UAV_vit.load_from_checkpoint(\"best_model.ckpt\")\n",
    "# model.freeze()\n",
    "\n",
    "# x = test_dataset[0][0]\n",
    "# predicition = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = curr_dataset[\"dataset\"][0][0]\n",
    "predicted = model(img.unsqueeze(0)).item()\n",
    "\n",
    "print(f\"Actual result {curr_dataset['dataset'][0][1]} predicted result {predicted}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
