
\begin{figure*}[t!]
    \includegraphics[width=\linewidth]{../images/ViT}
    \caption{ViT overview. An image is split into fixed-size patches, which are flattened and linearly projected into a sequence of vectors with positional embeddings. The sequence is then processed by a stack of transformer blocks, producing a sequence of vectors fed into a linear classifier. Illustration from \cite{dosovitskiy2021image}.}
    \label{fig:ViT}
    \centering
\end{figure*}

\section{Methods}
\label{sec:methods}

The use of UAVs in agriculture is not a new concept, but the introduction of VITs models has been a game changer in the field of Computer Vision (CV).
Unlike Multilayer Perceptrons (MLPs) and Convolutional Neural Networks (CNNs), ViTs excel at capturing intricate image patterns with their self-attention mechanism.
ViTs are highly scalable, reduce architectural complexity and support efficient transfer learning, therefore we decided to combine the two technologies to create a new solution for precision agriculture.

Our model design is based on the Vision Transformer architecture \cite{dosovitskiy2021image}.
In this section, we provide an overview of the architecture and the attention mechanism.

\subsection{Vision Transformer (ViT)}
\label{sec:ViT_overview}

The Vision Transformer is a variant of the transformer architecture, which was introduced by Vaswani et al. \cite{vaswani2023attention} for machine translation.

An overview of the model is depicted in fig.[\ref{fig:ViT}]. The standard Transformer receives as input a 1D sequence of token embeddings. To handle 2D images, the image $x \in \mathbb{R} ^{H\times W \times C}$ is reshaped into a sequence of flattened 2D patches $x_{p} \in \mathbb{R} ^{N\times(P^2\cdot C)}$, where $(H, W)$ is the resolution of the original image, $C$ is the number of channels, $(P, P)$ is the resolution of each image patch, and $N = HW/P^2$ is the resulting number of patches, which also serves as the effective input sequence length for the Transformer.

\begin{quote}
The Transformer uses constant latent vector size $D$ through all of its layers, so the patches are flattened and mapped to $D$ dimensions. The output of this projection is referred as the $patch$ $embeddings$.
Position embeddings are added to the patch embeddings to retain positional information. The resulting sequence of embedding vectors serves as input to the encoder\cite{dosovitskiy2021image}.
\end{quote}

The Transformer encoder \cite{vaswani2023attention} consists of alternating layers of Multiheaded Self-Attention (MSA) [\ref{subsec:MSA}] and MLP blocks. Layernorm (LN) \cite{ba2016layer} is applied before every block, and residual connections \cite{he2015deep} after every block.

\subsection{Multi-head self-attention (MSA)}
\label{subsec:MSA}

The attention mechanism is based on a trainable associative memory with (key, value) vector pairs.

A \textit{query} vector $q \in \mathbb{R} ^d$ is compared with each \textit{k key} vectors, packed into a matrix $K \in \mathbb{R} ^{k \times d}$, using inner products.
These inner products are then scaled and normalized with a softmax function to obtain $k$ weights.
The output of the attention is the weighted sum of a set of \textit{k value} vectors, packed into a matrix $V \in \mathbb{R} ^{k \times d}$.

For a sequence of \textit{N} query vectors packed into $Q \in \mathbb{R} ^{N \times d}$, it produces an output matrix (of size $N \times d$):
\begin{equation} \label{eq:attention} 
\text{Attention}(Q,K,V) = \text{Softmax}(QK^\intercal/\sqrt{d})V 
\end{equation}
where the Softmax function is applied to each row of the input matrix and the $\sqrt{d}$ term provides appropriate normalization. In \cite{vaswani2023attention} a Self-attention layer is proposed. Query, key and value matrices are themselves computed from a sequence of $N$ input vectors (packed into $X \in \mathbb{R} ^{N \times d}): Q = XW_{Q}, K = XW_{K}, V = XW_{V}$, by using linear transformations $W_{Q}, W_{K}, W_{V}$ with the constraint $k = N$, meaning that the attention is in between all the input vectors.
Finally, MSA layer is defined by considering $h$ attention $heads$, i.e., $h$ self-attention functions applied to the input. Each head provides a sequence of size $N \times d$.
These $h$ sequences are rearranged into a $N \times dh$ sequence that is reprojected by a linear layer into $N \times D$ \cite{touvron2021training}.

\subsection{Transfer Learning and Fine-tuning}
\label{subsec:TL-FT}

Our model implementation is based on the Lightning framework \cite{Falcon_PyTorch_Lightning_2019} which build upon \cite{paszke2019pytorch}.
It was pre-trained on a large image dataset\cite{deng2009imagenet} mostly used in classification tasks  and fine-tuned by using the plots images extracted from the orthomosaic. Pretraining is useful because it allows us to leverage knowledge learned from a large dataset and apply it to a different task.
In our case, we used the pre-trained model to extract features from ImageNet and then fine-tuned it to our specific task.

To tailor the model for our regression task, we replaced the backbone's $head$ with a linear layer, which outputs a single-value output.

We adopted the Log Cosh loss function\cite{moshagen2021finding}, which is more robust to outliers.
For small errors it behaves as Mean Squared Error (MSE) whilst for large errors behaves as the Mean Absolute Error (MAE).
The loss function is defined as:
\begin{equation}
\label{eq:logcosh}
\text{LogCosh}(y, \hat{y}) = \frac{1}{n} \sum_{n = 1}^{n} \log(\cosh(\hat{y} - y))
\end{equation}
The optimizer of choice is AdamW \cite{loshchilov2019decoupled}, which is a variant of Adam \cite{kingma2017adam} that uses weight decay regularization.
Hutter pointed out in their paper that the weight decay is implemented wrongly in Adam and proposed a way to fix it.
\begin{equation}
\label{eq:first_moment}
m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t
\end{equation}
\begin{equation}
\label{eq:second_moment}
v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2
\end{equation}
\begin{equation}
\label{eq:weight_update}
w_t = w_{t-1} - \frac{lr}{1 - \beta_1^t} \cdot \frac{m_t}{\sqrt{v_t} + \epsilon}
\end{equation}
where $m_t$ and $v_t$ are the first and second moments of the gradient, $w_t$ is the weight at time step $t$, $g_t$ is the derivative of the cost with respect to $w_t$, $\beta_1$ and $\beta_2$ are the exponential decay rates for the moment estimates, $lr$ is the learning rate, $\epsilon$ is a small constant to prevent division by zero.