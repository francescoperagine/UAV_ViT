\section{Experiments}
\label{sec:experiments}

In this section, we provide an overview of the experiments performed to assess the performance and robustness of the PyTorch ViT\_B\_16 model in the context of precision agriculture.

In our experimentation, we observed a tendency for the model to overfit, given the relatively small dataset. Overfitting occurs when models become overly specialized in the training data, which can compromise their ability to generalize effectively. To counter this, we explored an array of strategies and fine-tuned essential parameters. We also investigated alternative ways to optimize the hyperparameters and the model performace \cite{touvron2021training}, such as the Learning Rate finder \cite{smith2017cyclical} and alternate weights sets \cite{singh2022revisiting}.

Furthermore, as stated in \ref{sec:methods} we investigated the effect of different loss functions, including Huber Loss, Pseudo-Huber Loss and Log-Cosh Loss, aiming to enhance the model's performance. These experiments are supported by insightful visualizations, which help assess the model's behavior and predictive capabilities.

\begin{figure*}
    \begin{subfigure}{0.66\columnwidth}
        \includegraphics[width=\linewidth]{../images/outputs_vs_targets}
        \caption{Outputs vs. Targets}
        \label{fig:outputs_vs_targets}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.66\columnwidth}
        \includegraphics[width=\linewidth]{../images/residuals_errors}
        \caption{Residual Errors}
        \label{fig:residuals_errors}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.66\columnwidth}
        \includegraphics[width=\linewidth]{../images/residuals_vs_outputs}
        \caption{Residuals vs. Outputs}
        \label{fig:residuals_vs_outputs}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.66\columnwidth}
        \includegraphics[width=\linewidth]{../images/targets_means_vs_losses}
        \caption{Target Means vs. Losses}
        \label{fig:targets_means_vs_losses}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.66\columnwidth}
        \includegraphics[width=\linewidth]{../images/residuals_hist}
        \caption{Residual Histogram}
        \label{fig:residuals_hist}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.66\columnwidth}
        \includegraphics[width=\linewidth]{../images/boxplot}
        \caption{Boxplot}
        \label{fig:boxplot}
    \end{subfigure}
    \hfill
\end{figure*}

\subsection{Results}
\label{sec:results}

The residuals are randomly distributed around the zero line, suggesting that the model is doing a good job of fitting the data \ref{fig:residuals_vs_outputs}. 
Overall, the model does a good job of predicting values, as evidenced by the correlation between predicted and actual values \ref{fig:outputs_vs_targets}. 
However, there are a few data points that are outside the main cluster. These points may be outliers, which are data points that do not fit the overall pattern of the data.
In this case, due to the measurement of the ground truth taken three weeks earlier that the DSM imaging, the outliers are likely due to measurement errors. 

Results were not promising as the model was not able to strike a good R2Score. Across multiple hyperparameters configuration, the best R2Score result was was $\approx  0.27$ and it was observed with 9 frozen layers, LR =$1\exp{-6}$, Weight Decay = $1\exp{-1}$ and dropout $0.2$. This is likely due to the small dataset size, which is not enough to train a model with 86 million parameters. The model was also not able to converge, as the loss function was not able to reach a minimum value. This is likely due to the small learning rate, which was necessary to avoid overfitting.