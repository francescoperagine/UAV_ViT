{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Thuls\\Documents\\repository\\UAV_ViT\\venv\\Lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n",
      "c:\\Users\\Thuls\\Documents\\repository\\UAV_ViT\\venv\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:128: UserWarning: Field \"model_server_url\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Thuls\\Documents\\repository\\UAV_ViT\\venv\\Lib\\site-packages\\pydantic\\_internal\\_config.py:317: UserWarning: Valid config keys have changed in V2:\n",
      "* 'schema_extra' has been renamed to 'json_schema_extra'\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from torchmetrics import MetricCollection, MeanAbsoluteError, MeanSquaredError, ExplainedVariance, LogCoshError\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogCoshLoss(nn.Module):\n",
    "    def __init__:\n",
    "        super().__init__\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        error = y_pred - y_true\n",
    "        logcosh = torch.log(torch.cosh(error + 1e-12))\n",
    "        return torch.mean(logcosh, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UAV_vit(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, backbone, learning_rate, weight_decay, batch_size: int = 16, no_grad_layers_n: int = 6, dropout: float = 0.0, attention_dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.backbone = backbone\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.criterion = LogCoshLoss()\n",
    "        self.optimizer = AdamW(backbone.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        self.no_grad_layers_n = int(no_grad_layers_n)\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout\n",
    "        self.attention_dropout = attention_dropout\n",
    "\n",
    "        self.test_output = []\n",
    "        self.test_loss = []\n",
    "        self.test_targets_mean = []\n",
    "        # Set dropout\n",
    "        self.apply(lambda m: self.set_dropouts(m))\n",
    "\n",
    "        # Get the number of input features of the last layer of the backbone\n",
    "        num_input_filters = backbone.heads[0].in_features\n",
    "        num_output_values = 1\n",
    "\n",
    "        # Replace the head of the model\n",
    "        self.backbone.heads[0] = nn.Linear(in_features=num_input_filters, out_features=num_output_values).float()\n",
    "        \n",
    "        metric_collection = MetricCollection([\n",
    "            MeanSquaredError(),\n",
    "            MeanAbsoluteError(),\n",
    "            ExplainedVariance()\n",
    "        ])\n",
    "        self.val_metrics = metric_collection.clone(prefix=\"val_\")\n",
    "        self.test_metrics = metric_collection.clone(prefix=\"test_\")\n",
    "\n",
    "        if(self.no_grad_layers_n > 0):\n",
    "            for i, param in enumerate(self.backbone.encoder.parameters()):\n",
    "                if i < self.no_grad_layers_n:\n",
    "                    param.requires_grad = False\n",
    "\n",
    "    def set_dropouts(self, m):\n",
    "        if isinstance(m, nn.Dropout):\n",
    "            m.p = self.dropout\n",
    "        elif isinstance(m, nn.MultiheadAttention):\n",
    "            m.dropout = self.attention_dropout   \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer\n",
    "    \n",
    "    def get_batch_data(self, batch):\n",
    "        images, labels = batch\n",
    "        labels = labels.unsqueeze(1)\n",
    "        outputs = self.forward(images)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "\n",
    "        return outputs, labels, loss\n",
    "    \n",
    "    # Training\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        _, _, loss = self.get_batch_data(batch)\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, on_step=False)\n",
    "        return loss\n",
    "    \n",
    "    # Validation\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs, labels, loss = self.get_batch_data(batch)\n",
    "        step_metrics = self.val_metrics.forward(outputs, labels)\n",
    "        self.log_dict(step_metrics, on_epoch=True, on_step=False)\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, on_step=False)\n",
    "        # return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self.val_metrics.reset()\n",
    "\n",
    "    # Testing\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        outputs, labels, loss = self.get_batch_data(batch)\n",
    "        step_metrics = self.test_metrics.forward(outputs, labels)\n",
    "\n",
    "        outputs = outputs.squeeze().cpu().numpy()\n",
    "        labels = labels.squeeze().cpu().numpy()\n",
    "\n",
    "        self.test_output.extend(outputs)\n",
    "        self.test_loss.append(loss.item())\n",
    "        self.test_targets_mean.append(np.mean(labels))\n",
    "\n",
    "        self.log_dict(step_metrics, on_epoch=True, on_step=False)\n",
    "        self.log(\"test_loss\", loss, on_epoch=True, on_step=False)\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        self.test_metrics.reset()\n",
    "\n",
    "    # Prediction\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=None):\n",
    "        outputs = self(batch)\n",
    "        return outputs\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
