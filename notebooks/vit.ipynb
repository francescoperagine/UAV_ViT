{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "from torchmetrics import MetricCollection, MeanAbsoluteError, MeanSquaredError, ExplainedVariance\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UAV_vit(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, backbone, criterion, optimizer, batch_size: int = 16, no_grad_layers_n: int = 6, dropout: float = 0.0, attention_dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "                \n",
    "        self.backbone = backbone\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.no_grad_layers_n = int(no_grad_layers_n)\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout\n",
    "        self.attention_dropout = attention_dropout\n",
    "\n",
    "        self.test_output = []\n",
    "        self.test_loss = []\n",
    "        self.test_label_mean = []\n",
    "\n",
    "        # Set dropout\n",
    "        self.set_dropouts()\n",
    "\n",
    "        # Get the number of input features of the last layer of the backbone\n",
    "        num_input_filters = backbone.heads[0].in_features\n",
    "        num_output_values = 1\n",
    "\n",
    "        # Replace the head of the model\n",
    "        self.backbone.heads[0] = nn.Linear(in_features=num_input_filters, out_features=num_output_values).float()\n",
    "        \n",
    "        metric_collection = MetricCollection([\n",
    "            MeanSquaredError(),\n",
    "            MeanAbsoluteError(),\n",
    "            ExplainedVariance()\n",
    "        ])\n",
    "        self.val_metrics = metric_collection.clone(prefix=\"val_\")\n",
    "        self.test_metrics = metric_collection.clone(prefix=\"test_\")\n",
    "\n",
    "        if(self.no_grad_layers_n > 0):\n",
    "            for i, param in enumerate(self.backbone.encoder.parameters()):\n",
    "                if i < self.no_grad_layers_n:\n",
    "                    param.requires_grad = False\n",
    "\n",
    "    def set_dropouts(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Dropout):\n",
    "                m.p = self.dropout\n",
    "            elif isinstance(m, nn.MultiheadAttention):\n",
    "                m.dropout = self.attention_dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer\n",
    "    \n",
    "    # Training\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        _, _, loss = self.get_batch_data(batch)\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, on_step=False)\n",
    "        return loss\n",
    "    \n",
    "    # Validation\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs, labels, loss = self.get_batch_data(batch)\n",
    "        step_metrics = self.val_metrics.forward(outputs, labels)\n",
    "        self.log_dict(step_metrics, on_epoch=True, on_step=False)\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, on_step=False)\n",
    "        # return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self.val_metrics.reset()\n",
    "\n",
    "    # Testing\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        outputs, labels, loss = self.get_batch_data(batch)\n",
    "        step_metrics = self.test_metrics.forward(outputs, labels)\n",
    "\n",
    "        outputs = outputs.squeeze().cpu().numpy()\n",
    "        labels = labels.squeeze().cpu().numpy()\n",
    "\n",
    "        self.test_output.extend(outputs)\n",
    "        self.test_loss.append(loss.item())\n",
    "        self.test_label_mean.append(np.mean(labels))\n",
    "\n",
    "        self.log_dict(step_metrics, on_epoch=True, on_step=False)\n",
    "        self.log(\"test_loss\", loss, on_epoch=True, on_step=False)\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        self.test_metrics.reset()\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=None):\n",
    "        outputs = self.forward(batch)\n",
    "        predictions = [x.item() for x in outputs.detach().cpu().numpy()]\n",
    "        return predictions\n",
    "\n",
    "    def get_batch_data(self, batch):\n",
    "        images, labels = batch\n",
    "        labels = labels.unsqueeze(1)\n",
    "        outputs = self.forward(images)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "\n",
    "        return outputs, labels, loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
