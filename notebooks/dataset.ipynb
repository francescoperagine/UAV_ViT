{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Repository\\UAV_ViT\\venv\\lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "d:\\Repository\\UAV_ViT\\venv\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import v2\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as TF\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquarePad:\n",
    "    def __call__(self, image):\n",
    "        _, w, h = image.size()\n",
    "        max_wh = max(w, h)\n",
    "        hp = int((max_wh - w) / 2)\n",
    "        vp = int((max_wh - h) / 2)\n",
    "        padding = [vp, vp, hp, hp]\n",
    "        padded_img = TF.pad(image, padding)\n",
    "        return padded_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxScalerVectorized(object):\n",
    "    def __call__(self, image):\n",
    "        dist = (image.max(dim=1, keepdim=True)[0] - image.min(dim=1, keepdim=True)[0])\n",
    "        dist[dist==0.] = 1.\n",
    "        scale = 1.0 /  dist\n",
    "        image.mul_(scale).sub_(image.min(dim=1, keepdim=True)[0])\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset):\n",
    "    def __init__(self, img_dir, img_size, antialias=True):\n",
    "        self.img_dir = img_dir        \n",
    "        self.img_size = img_size\n",
    "        self.antialias = antialias\n",
    "        self.image_names = os.listdir(img_dir)\n",
    "        \n",
    "        self.transform = v2.Compose([\n",
    "            # Initialize the transform list by adding in the square padding\n",
    "            SquarePad(),\n",
    "            v2.Resize(size=self.img_size, antialias=self.antialias),\n",
    "            # Append the conversion to tensor to the transform list\n",
    "            v2.ConvertImageDtype(torch.float32),\n",
    "            MinMaxScalerVectorized()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.image_names[index] # picks all images in the directory\n",
    "\n",
    "        image_path = os.path.join(os.getcwd(), self.img_dir, row)\n",
    "        image = read_image(image_path)\n",
    "        \n",
    "        image = self.transform(image)\n",
    "\n",
    "        return image\n",
    "    \n",
    "    def save_dataset(self, filename):\n",
    "        with open(filename, 'wb') as f:\n",
    "            torch.save(self, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlotsDataset(BaseDataset):\n",
    "\n",
    "    \"\"\" Args: \n",
    "            labels: dataframe with the labels must contain the columns: filename, elevation\n",
    "            img_size: tuple with the size of the image\"\"\"\n",
    "\n",
    "    def __init__(self, labels, *args, **kwargs):\n",
    "        self.labels = labels        \n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.labels.iloc[index] # picks only those in the ground truth\n",
    "\n",
    "        image_path = os.path.join(os.getcwd(), self.img_dir, row['filename'])\n",
    "        image = read_image(image_path)\n",
    "        image = self.transform(image)\n",
    "\n",
    "        label = row['elevation']\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def show_samples(self, labels, name):\n",
    "        cols, rows = 4, 4\n",
    "        figure = plt.figure(figsize=(8, 8))\n",
    "        for i in range(1, cols * rows + 1):\n",
    "            sample_index = torch.randint(len(self), size=(1,)).item()\n",
    "            img, _ = self[sample_index]\n",
    "            label = labels[sample_index]\n",
    "            img = img.permute(1,2,0)\n",
    "            figure.add_subplot(rows, cols, i)\n",
    "            plt.suptitle(\"Plot samples \" + name)\n",
    "            filename = self.labels.iloc[i]['filename'][0:-4]\n",
    "            plt.title(f'{filename}: {label:.2f}m')        \n",
    "            plt.axis(\"off\")\n",
    "            plt.imshow(img)\n",
    "        plt.show()\n",
    "\n",
    "    def get_means_stds(self):\n",
    "        tensors = [img for img, _ in self]\n",
    "\n",
    "        # Split channels\n",
    "        channels = torch.chunk(torch.stack(tensors), 3, dim=1)\n",
    "\n",
    "        means = [torch.mean(channel).item() for channel in channels]\n",
    "        stds = [torch.std(channel).item() for channel in channels]\n",
    "\n",
    "        return means, stds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
