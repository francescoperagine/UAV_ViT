{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Repository\\UAV_ViT\\venv\\lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "d:\\Repository\\UAV_ViT\\venv\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as TF\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquarePad:\n",
    "    def __call__(self, image):\n",
    "        _, w, h = image.size()\n",
    "        max_wh = max(w, h)\n",
    "        hp = int((max_wh - w) / 2)\n",
    "        vp = int((max_wh - h) / 2)\n",
    "        padding = [vp, vp, hp, hp]\n",
    "        padded_img = TF.pad(image, padding)\n",
    "        return padded_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlotsDataset(Dataset):\n",
    "    def __init__(self, labels, img_dir, img_size, transforms = None, antialias = True):\n",
    "        self.labels = labels\n",
    "        self.img_dir = img_dir\n",
    "        self.img_size = img_size\n",
    "        self.antialias = antialias\n",
    "        \n",
    "        # Initialize the transform list by adding in the square padding\n",
    "        compose_square_pad = v2.Compose([ SquarePad() ])\n",
    "        compose_resize_convert = v2.Compose([\n",
    "            v2.Resize(size=self.img_size, antialias=self.antialias),\n",
    "            v2.ToImageTensor(),\n",
    "            v2.ConvertImageDtype(torch.float32),\n",
    "            # v2.Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])\n",
    "        ])\n",
    "        # compose_normalization = v2.Compose([v2.Normalize(mean=self.means, std=self.stds)])\n",
    "\n",
    "        self.transform = compose_square_pad\n",
    "\n",
    "        # Add in any user-specified transforms\n",
    "        self.transform = v2.Compose([self.transform, transforms]) if(transforms is not None) else self.transform\n",
    "\n",
    "        # Add in the rest of the transforms\n",
    "        self.transform = v2.Compose([self.transform, compose_resize_convert])\n",
    "\n",
    "        # self.means, self.stds = self.calculate_mean_std()\n",
    "        # print(f\"Means: {self.means}, Stds: {self.stds}\")\n",
    "\n",
    "        # Normalize based on dataset mean and standard deviation\n",
    "        # self.transform = v2.Compose([self.transform, compose_normalization])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.labels.iloc[index]\n",
    "\n",
    "        image_path = os.path.join(os.getcwd(), self.img_dir, row['plot'])\n",
    "        image = read_image(image_path)\n",
    "        image = self.transform(image)\n",
    "\n",
    "        label = row['elev']\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def save_dataset(self, filename):\n",
    "        with open(filename, 'wb') as f:\n",
    "            torch.save(self, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
